{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBbM8tWpZ5kF"
      },
      "source": [
        "### Just a quick example of code that runs through all images in validation set and calculates competition metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIL_GNkz37t2"
      },
      "outputs": [],
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IhufF3iZ5kH"
      },
      "outputs": [],
      "source": [
        "# IPATH = '/content/drive/MyDrive/Kaggle/Sartorius/detectron2/whls'\n",
        "# !pip install {IPATH}/pycocotools-2.0.2/dist/pycocotools-2.0.2.tar --no-index --find-links /content/drive/MyDrive/Kaggle/Sartorius/detectron2/whls\n",
        "# !pip install {IPATH}/fvcore-0.1.5.post20211019/fvcore-0.1.5.post20211019 --no-index --find-links /content/drive/MyDrive/Kaggle/Sartorius/detectron2/whls\n",
        "# !pip install   {IPATH}/antlr4-python3-runtime-4.8/antlr4-python3-runtime-4.8 --no-index --find-links /content/drive/MyDrive/Kaggle/Sartorius/detectron2/whls\n",
        "# !pip install {IPATH}/detectron2-0.5/detectron2 --no-index --find-links /content/drive/MyDrive/Kaggle/Sartorius/detectron2/whls "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp-m1qc7tzYd"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/drive/MyDrive/Kaggle/Sartorius')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjYGZfLjZ5kI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.modeling import GeneralizedRCNNWithTTA\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "import cv2\n",
        "import os\n",
        "import pycocotools.mask as mask_util\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Any, Iterator, List, Union\n",
        "import numpy as np\n",
        "import detectron2\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from types import SimpleNamespace\n",
        "from detectron2.structures.masks import ROIMasks\n",
        "from detectron2.layers.mask_ops import paste_masks_in_image\n",
        "from detectron2.evaluation import DatasetEvaluators\n",
        "from detectron2.evaluation.evaluator import DatasetEvaluator\n",
        "from detectron2.evaluation import inference_on_dataset\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from scipy.ndimage.morphology import binary_fill_holes\n",
        "from skimage.morphology import dilation, erosion\n",
        "from skimage.measure import label\n",
        "#from utils.post_processing import compute_overlaps_masks\n",
        "from fvcore.transforms import HFlipTransform, NoOpTransform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NdUqvngwMwO"
      },
      "source": [
        "# Post Processing Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1Ra6lkMwMfb"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "\n",
        "  THRESHOLDS = [.34646999695500497, .3134462119696301, .6716982906416183]#[0.2848161999265901,0.5901576776244156,0.6508687839015181]\n",
        "  MIN_PIXELS = [60, 120, 60]#[60,80,50]\n",
        "  # 该版本中以下都没用到\n",
        "  PIXEL_CLASS_THRESHOLDS = [0.3, 0.25, 0.5]\n",
        "  NMS_THRESH_TEST = 0.25\n",
        "\n",
        "  anchor_generators_sizes = [[9], [17], [31], [64],[127]]\n",
        "  anchor_generators_aspect_ratios = [[0.5, 1.0, 2.0]]\n",
        "\n",
        "  pixel_mean = [128,128,128]\n",
        "  pixel_std = [13.235,13.235,13.235]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhBUzNcRuRjr"
      },
      "source": [
        "# Custom TTA\n",
        "\n",
        "需要custom一下tta的cnn类和transform 加入旋转"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhtNKMa1uRRb"
      },
      "outputs": [],
      "source": [
        "class CustomizedRCNNWithTTA(GeneralizedRCNNWithTTA):\n",
        "  def _reduce_pred_masks(self, outputs, tfms):\n",
        "    # Should apply inverse transforms on masks.\n",
        "    # We assume only resize & flip are used. pred_masks is a scale-invariant\n",
        "    # representation, so we handle flip specially\n",
        "    for output, tfm in zip(outputs, tfms):\n",
        "        if any(isinstance(t, HFlipTransform) for t in tfm.transforms):\n",
        "            output.pred_masks = output.pred_masks.flip(dims=[3])\n",
        "    all_pred_masks = [np.transpose(o.pred_masks.cpu().numpy().squeeze(),(1,2,0)) for o in outputs]\n",
        "    # voting\n",
        "    overlaps = compute_overlaps_masks(all_pred_masks[0],all_pred_masks[1])\n",
        "\n",
        "    for mm in range(overlaps.shape[0]):\n",
        "        if np.max(overlaps[mm]) > 0.1:\n",
        "            ind = np.argmax(overlaps[mm])\n",
        "            all_pred_masks[0][:, :, mm] = (all_pred_masks[0][:, :, mm] + all_pred_masks[1][:, :, ind])/2\n",
        "            #all_pred_masks[0][:, :, mm] = (mask > 0).astype(np.uint8)\n",
        "            #result1['scores'][mm] = 0.5*(result1['scores'][mm]+result2['scores'][ind])\n",
        "        else:\n",
        "            all_pred_masks[0][:, :, mm] = 0\n",
        "    all_pred_masks = [np.expand_dims(np.transpose(p,(2,0,1)),1) for p in all_pred_masks]\n",
        "    return torch.tensor(all_pred_masks[0]).cuda()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Om1k9FGuMrG"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "334NYOeP8GrU"
      },
      "outputs": [],
      "source": [
        "# 安装了2.0.5的detectron后 以下代码可以获取每个像素是否为mask的概率 而不是0/1\n",
        "# def paste_masks_in_image(masks, boxes, image_shape, threshold=0.5):\n",
        "#     \"\"\"\n",
        "#     Copy pasted from detectron2.layers.mask_ops.paste_masks_in_image and deleted thresholding of the mask\n",
        "#     \"\"\"\n",
        "#     assert masks.shape[-1] == masks.shape[-2], \"Only square mask predictions are supported\"\n",
        "#     N = len(masks)\n",
        "#     if N == 0:\n",
        "#         return masks.new_empty((0,) + image_shape, dtype=torch.uint8)\n",
        "#     if not isinstance(boxes, torch.Tensor):\n",
        "#         boxes = boxes.tensor\n",
        "#     device = boxes.device\n",
        "#     assert len(boxes) == N, boxes.shape\n",
        "\n",
        "#     img_h, img_w = image_shape\n",
        "\n",
        "#     # The actual implementation split the input into chunks,\n",
        "#     # and paste them chunk by chunk.\n",
        "#     if device.type == \"cpu\":\n",
        "#         # CPU is most efficient when they are pasted one by one with skip_empty=True\n",
        "#         # so that it performs minimal number of operations.\n",
        "#         num_chunks = N\n",
        "#     else:\n",
        "#         # GPU benefits from parallelism for larger chunks, but may have memory issue\n",
        "#         num_chunks = int(np.ceil(N * img_h * img_w * BYTES_PER_FLOAT / GPU_MEM_LIMIT))\n",
        "#         assert (\n",
        "#             num_chunks <= N\n",
        "#         ), \"Default GPU_MEM_LIMIT in mask_ops.py is too small; try increasing it\"\n",
        "#     chunks = torch.chunk(torch.arange(N, device=device), num_chunks)\n",
        "\n",
        "#     img_masks = torch.zeros(\n",
        "#         N, img_h, img_w, device=device, dtype=torch.float32\n",
        "#     )\n",
        "#     for inds in chunks:\n",
        "#         masks_chunk, spatial_inds = _do_paste_mask(\n",
        "#             masks[inds, None, :, :], boxes[inds], img_h, img_w, skip_empty=device.type == \"cpu\"\n",
        "#         )\n",
        "#         img_masks[(inds,) + spatial_inds] = masks_chunk\n",
        "#     return img_masks\n",
        "\n",
        "# detectron2.layers.mask_ops.paste_masks_in_image.__code__ = paste_masks_in_image.__code__\n",
        "\n",
        "# def BitMasks__init__(self, tensor: Union[torch.Tensor, np.ndarray]):\n",
        "#   \"\"\"\n",
        "#   Args:\n",
        "#   tensor: bool Tensor of N,H,W, representing N instances in the image.\n",
        "#   \"\"\"\n",
        "#   device = tensor.device if isinstance(tensor, torch.Tensor) else torch.device(\"cpu\")\n",
        "#   tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device) # Original code: tensor = torch.as_tensor(tensor, dtype=torch.bool, device=device)\n",
        "#   assert tensor.dim() == 3, tensor.size()\n",
        "#   self.image_size = tensor.shape[1:]\n",
        "#   self.tensor = tensor\n",
        "\n",
        "# detectron2.structures.masks.BitMasks.__init__.__code__ = BitMasks__init__.__code__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kxhKahaZ5kJ"
      },
      "outputs": [],
      "source": [
        "def precision_at(threshold, iou):\n",
        "    matches = iou > threshold\n",
        "    try:\n",
        "      true_positives = np.sum(matches, axis=1) >= 1  # Correct objects\n",
        "      false_positives = np.sum(matches, axis=1) == 0  # Extra objects\n",
        "      false_negatives = np.sum(matches, axis=0) == 0  # Missed objects\n",
        "      return np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
        "    except:\n",
        "      return 0,1,1\n",
        "\n",
        "\n",
        "def score(pred, targ):\n",
        "    enc_preds = post_process(pred)\n",
        "    enc_preds = [mask_util.encode(np.asarray(p, order='F')) for p in enc_preds]\n",
        "    enc_targs = list(map(lambda x:x['segmentation'], targ))\n",
        "    ious = mask_util.iou(enc_preds, enc_targs, [0]*len(enc_targs))\n",
        "    prec = []\n",
        "    for t in np.arange(0.5, 1.0, 0.05):\n",
        "        tp, fp, fn = precision_at(t, ious)\n",
        "        p = tp / (tp + fp + fn)\n",
        "        prec.append(p)\n",
        "        class_score[int(torch.mode(pred['instances'].pred_classes)[0])].append(p)\n",
        "        #print(\"-\"*50)\n",
        "        #print(f\"When threshold is {t}||Score is {p}||TP is {tp}||FP is {fp}||FN is {fn}\")\n",
        "    return np.mean(prec)\n",
        "\n",
        "def rle_decode(mask_rle, shape=(520, 704)):\n",
        "    '''\n",
        "    mask_rle: run-length as string formated (start length)\n",
        "    shape: (height,width) of array to return \n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "\n",
        "    '''\n",
        "    s = mask_\n",
        "    rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape)  # Needed to align to RLE direction\n",
        "\n",
        "def rle_encode(img):\n",
        "    '''\n",
        "    img: numpy array, 1 - mask, 0 - background\n",
        "    Returns run length as string formated\n",
        "    '''\n",
        "    pixels = img.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return [x for x in runs]#' '.join(str(x) for x in runs)\n",
        "\n",
        "def post_process(pred):\n",
        "    pred_class = torch.mode(pred['instances'].pred_classes)[0]\n",
        "    take = pred['instances'].scores >= Config.THRESHOLDS[pred_class]\n",
        "    pred_masks = pred['instances'].pred_masks[take].cpu().numpy()\n",
        "\n",
        "    res = []\n",
        "    used = np.zeros(pred['instances'].pred_masks[0].shape)\n",
        "\n",
        "    for idx,mask in enumerate(pred_masks):\n",
        "        # 先填洞，筛选再去重\n",
        "        #mask = binary_fill_holes(mask).astype(np.uint8)\n",
        "        #mask = erosion(dilation(mask))\n",
        "\n",
        "        # if mask.sum() >= Config.MIN_PIXELS[pred_class]: # skip predictions with small area\n",
        "        #     #Delete overlaps\n",
        "        #     used += mask\n",
        "        #     mask[used > 1] = 0\n",
        "        #     out_label = label(mask)\n",
        "        #     # Remove all the pieces if there are more than one pieces\n",
        "        #     if out_label.max() > 1:\n",
        "        #         mask[()] = 0\n",
        "        #     res.append(mask.astype(bool))\n",
        "\n",
        "        # 去重后筛选\n",
        "        mask = mask * (1-used)\n",
        "        if mask.sum() >= Config.MIN_PIXELS[pred_class]: # skip predictions with small area\n",
        "            used += mask\n",
        "            res.append(mask.astype(bool))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lotQl7yWpNEU"
      },
      "source": [
        "# Generalized RCNN TTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfxJ0O-v26Kt"
      },
      "outputs": [],
      "source": [
        "class MAPIOUEvaluator(DatasetEvaluator):\n",
        "    def __init__(self, dataset_name):\n",
        "        dataset_dicts = DatasetCatalog.get(dataset_name)\n",
        "        self.annotations_cache = {item['image_id']:item['annotations'] for item in dataset_dicts}\n",
        "            \n",
        "    def reset(self):\n",
        "        self.scores = []\n",
        "\n",
        "    def process(self, inputs, outputs):\n",
        "        for inp, out in zip(inputs, outputs):\n",
        "            if len(out['instances']) == 0:\n",
        "                self.scores.append(0)    \n",
        "            else:\n",
        "\n",
        "                targ = self.annotations_cache[inp['image_id']]\n",
        "                self.scores.append(score(out, targ))\n",
        "\n",
        "    def evaluate(self):\n",
        "   \n",
        "        return {\"MaP IoU\": np.mean(self.scores)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPcaXuSMpPl2"
      },
      "outputs": [],
      "source": [
        "class Trainer(DefaultTrainer):\n",
        "    \"\"\"\n",
        "    We use the \"DefaultTrainer\" which contains pre-defined default logic for\n",
        "    standard training workflow. They may not work for you, especially if you\n",
        "    are working on a new research project. In that case you can write your\n",
        "    own training loop. You can use \"tools/plain_train_net.py\" as an example.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        return MAPIOUEvaluator(dataset_name)\n",
        "\n",
        "    @classmethod\n",
        "    def test_with_TTA(cls, cfg, model):\n",
        "        # TTA+投票法 暂时不需要\n",
        "        #model = CustomizedRCNNWithTTA(cfg, model)\n",
        "\n",
        "        # 需要TTA时打开以下这行的注释\n",
        "        #model = GeneralizedRCNNWithTTA(cfg, model)\n",
        "\n",
        "        # 无TTA时 屏蔽上述两行用\n",
        "        evaluator = [cls.build_evaluator(\n",
        "            cfg, 'sartorius_val', # output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n",
        "        )]\n",
        "\n",
        "        data_loader = cls.build_test_loader(cfg, 'sartorius_val')\n",
        "\n",
        "        results_i = inference_on_dataset(model, data_loader, evaluator)\n",
        "        return results_i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5akCaB97uUeW"
      },
      "source": [
        "# Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-fOlwblldoG"
      },
      "outputs": [],
      "source": [
        "def run(model_config,model_path,val_path,):\n",
        "  cfg = get_cfg()\n",
        "  cfg.INPUT.MASK_FORMAT='bitmask'\n",
        "  cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3 \n",
        "  cfg.MODEL.WEIGHTS = model_path\n",
        "  cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
        "  cfg.DATALOADER.NUM_WORKERS = 2\n",
        "  cfg.OUTPUT_DIR = \"output/\"\n",
        "  cfg.DATASETS.TEST = ('sartorius_val',)\n",
        "\n",
        "  # TTA设置\n",
        "  cfg.TEST.AUG.ENABLED = True\n",
        "  cfg.TEST.AUG.MIN_SIZES = (1040,)\n",
        "  cfg.TEST.AUG.FLIP = True\n",
        "  #############################################################################\n",
        "  # Cascade Team\n",
        "  cfg.merge_from_file(model_zoo.get_config_file(model_config))\n",
        "  cfg.MODEL.WEIGHTS = model_path #这个是用livecell迭代6503次的预训练权重\n",
        "  cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "  cfg.INPUT.MIN_SIZE_TEST = 1040\n",
        "  cfg.INPUT.MAX_SIZE_TEST = 1408\n",
        "  cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.35\n",
        "  #cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256 \n",
        "  cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  \n",
        "  #cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .5\n",
        "  #############################################################################\n",
        "\n",
        "\n",
        "  cfg.freeze()\n",
        "  \n",
        "  DatasetCatalog.clear()\n",
        "  MetadataCatalog.clear()\n",
        "\n",
        "  register_coco_instances('sartorius_val',{},val_path, \n",
        "                          '/content/drive/MyDrive/Kaggle/Sartorius/input/sartorius-cell-instance-segmentation')\n",
        "\n",
        "  val_ds = DatasetCatalog.get('sartorius_val')\n",
        "\n",
        "  model = Trainer.build_model(cfg)\n",
        "\n",
        "  DetectionCheckpointer(model, save_dir=model_path).resume_or_load(\n",
        "      cfg.MODEL.WEIGHTS, resume=False\n",
        "  )\n",
        "\n",
        "  scr = Trainer.test_with_TTA(cfg, model)\n",
        "\n",
        "  return scr[\"MaP IoU\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss3IByXH00T8"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/Kaggle/Sartorius/model/TEAM/cascade_cv316/1218_cv316.pth\"#\"/content/drive/MyDrive/Kaggle/Sartorius/model/finetuned/cascade_v0/fold_1/model_best.pth\"#'/content/drive/MyDrive/Kaggle/Sartorius/model/finetuned/cascade_v0/fold_2/model_best.pth' \n",
        "model_config = \"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml\" ##\"Misc/cascade_mask_rcnn_R_50_FPN_3x.yaml\"\n",
        "val_path = \"/content/drive/MyDrive/Kaggle/Sartorius/input/all/annotations_val.json\"#/content/drive/MyDrive/Kaggle/Sartorius/input/fold/coco_cell_valid_fold1.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izD_EAtOxgzi"
      },
      "outputs": [],
      "source": [
        "class_score = {0:[],1:[],2:[]} #shy5sy, astro, cort\n",
        "run(model_config,model_path,val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrwKYpG0U675",
        "outputId": "c5d3e462-a20e-4609-b2ff-e17a86afaca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0 is  0.22050477051156162\n",
            "Class 1 is  0.17676962157585002\n",
            "Class 2 is  0.3932212240991607\n"
          ]
        }
      ],
      "source": [
        "# 显示不同种类细胞的mAP\n",
        "for i in range(3):\n",
        "  print(f\"Class {i} is \",np.mean(class_score[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr3-734dpK1L"
      },
      "source": [
        "# OPTUNA\n",
        "\n",
        "调参用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_9HWlNQsuTQ"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er9F7c58pMqb"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "# parameters search\n",
        "class params_optim():\n",
        "  def __init__(self,train_time):\n",
        "    self.train_time = train_time\n",
        "  \n",
        "  def search(self):\n",
        "\n",
        "    study = optuna.create_study(study_name=\"Post_Processing_Parameters\",direction=\"maximize\",sampler=TPESampler())\n",
        "    study.optimize(self.objective,n_trials=self.train_time)\n",
        "    best_trial = study.trial\n",
        "    print(best_trial)\n",
        "    return best_trial.params\n",
        "\n",
        "  def objective(self,trial):\n",
        "    params = {\"threshold_class_1\":trial.suggest_uniform(\"threshold_class_1\",0.1,0.5),\n",
        "              \"threshold_class_2\":trial.suggest_uniform(\"threshold_class_2\",0.3,0.7),\n",
        "              \"threshold_class_3\":trial.suggest_uniform(\"threshold_class_3\",0.4,0.8),\n",
        "\n",
        "              \"min_pixel_class_1\":trial.suggest_int(\"min_pixel_class_1\",30,100,10),\n",
        "              \"min_pixel_class_2\":trial.suggest_int(\"min_pixel_class_2\",40,200,20),\n",
        "              \"min_pixel_class_3\":trial.suggest_int(\"min_pixel_class_3\",30,100,10),\n",
        "              }\n",
        "\n",
        "    Config.MIN_PIXELS = [params[\"min_pixel_class_1\"], params[\"min_pixel_class_2\"], params[\"min_pixel_class_3\"]] # shsy5y, astro, cort\n",
        "\n",
        "    scr = run(model_config,model_path,val_path)\n",
        "\n",
        "    return scr\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5Y9pwg_snGw"
      },
      "outputs": [],
      "source": [
        "# 进行120次实验\n",
        "params = params_optim(120).search()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "validation_v0.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}